---
title: "Mulitiplicative_SBF"
output: html_document
date: "2025-12-12"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# R_code_description

This file should produce the same plots as the paper

## Load packages

```{r}
graphics.off()
library(parallel) 
library("MASS") 
```

## Parameters set

This set of parameters corresponds to the most challenging simulation setup in Section 5 of the paper: Model 2, sample size 200, highly correlated covariates(0.8), and the differences in numerical stability and ISE between smooth backfitting and the Lin–He–Huang (2016) method are compared through 200 Monte Carlo repetitions.

```{r}
d_par <- c(3)       # the dimension of data, including time (time + z1 + z2)
rho_par <- c(0.8)   # the correlation between Z tilde
n_par <- c(200)     # the number of observations of data
model_par <- c(2)   # model2
n_sim_par <- c(200) # number of simulation
n_cores <- c(7)
r <- 1:n_sim_par    # list for index

```


## Seeds set

```{r}
set.seed(123)
jobs <- expand.grid(d_par, rho_par, model_par, n_par, n_sim_par, n_cores, r) ## dataframe for running
jobs <- lapply(1:nrow(jobs), function(x){list(jobs=jobs[x,],seeds=round(runif(n_sim_par,0,999999999)))}) ## assign different seeds
```


## Main function

### smooth.alpha funcion

The multiplicative hazard model is
\[
\alpha(t, z) = \alpha_0(t) \prod_{k=1}^{d-1} \alpha_k(z_k).
\]

Each component \(\alpha_k\) is estimated on a grid:
\[
\{x_{k1}, x_{k2}, \dots, x_{k m_k}\}.
\]

To evaluate the function at an observed covariate value \(X_{ik}\),
a Nadaraya–Watson type kernel smoother is used:
\[
\hat{\alpha}_k(X_{ik})
=
\frac{
\sum_{\ell=1}^{m_k}
K\!\left(\frac{X_{ik}-x_{k\ell}}{b}\right)
\hat{\alpha}_k(x_{k\ell})
}{
\sum_{\ell=1}^{m_k}
K\!\left(\frac{X_{ik}-x_{k\ell}}{b}\right)
}.
\]

The baseline hazard \(\alpha_0(t)\) is treated separately because it is integrated over time
and interacts with the risk indicator \(Y_i(t)\).

---
Function Arguments and Their Meaning

  alpha,        # list of grid-based estimates alpha[[k]], the vector of length n.gird[k]

  K.X.b,        # list of kernel weight matrices (sample -> grid)

  k.X.b,        # list of kernel weight sums (normalization)

  K.b,          # kernel smoothing matrix for time grid (baseline)

  k.b,          # normalization vector for baseline smoothing

  b,            # bandwidth (not explicitly used here)

  n.grid,       # number of grid points per dimension

  d,            # total dimension (time + covariates)

  n,            # sample size

  nb,           # number of bandwidths (technical)

  get.baseline  # logical: whether baseline hazard is estimated



```{r}

myparallelfunction <- function(x){
  smooth.alpha <- function(alpha, K_X_b,k_X_b, K_b,k_b, b, n_grid, d,n,nb, get_baseline ){
    alpha_smooth_i <- array(dim = c(d,n))
    alpha_smooth_i_0 <- array(dim = c(n,n_grid[1]))
    for ( k in 1:d ) {
      if (k==1){
        for (i in 1:n) {
          alpha_smooth_i[k,i] <- 1
          alpha_smooth_i_0[i,] <- (K_b/k_b)%*%alpha[[k]]}  # no weight dx because they are the same and constant, can be cancelled (TODO)
      } else for (i in 1:n) {
      alpha_smooth_i[k,i] <- (K_X_b[[k]][i,]/k_X_b[[k]][i])%*%alpha[[k]] 
      }
    
    if (get_baseline==FALSE) return(alpha_smooth_i) else return(list(alpha_smooth_i = alpha_smooth_i, alpha_smooth_i_0 = alpha_smooth_i_0))
    }
  
  get.denominator <- function(){}
  }
}
```




