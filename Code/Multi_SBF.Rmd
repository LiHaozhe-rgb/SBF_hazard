---
title: "Mulitiplicative_SBF"
output: html_document
date: "2025-12-12"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# R_code_description

This file should produce the same plots as the paper

## Load packages

```{r}
graphics.off()
library(parallel) 
library("MASS") 
```

## Parameters set

This set of parameters corresponds to the most challenging simulation setup in Section 5 of the paper: Model 2, sample size 200, highly correlated covariates(0.8), and the differences in numerical stability and ISE between smooth backfitting and the Lin–He–Huang (2016) method are compared through 200 Monte Carlo repetitions.

```{r}
d_par <- c(3)       # the dimension of data, including time (time + z1 + z2)
rho_par <- c(0.8)   # the correlation between Z tilde
n_par <- c(200)     # the number of observations of data
model_par <- c(2)   # model2
n_sim_par <- c(200) # number of simulation
n_cores <- c(7)
r <- 1:n_sim_par    # list for index

```


## Seeds set

```{r}
set.seed(123)
jobs <- expand.grid(d_par, rho_par, model_par, n_par, n_sim_par, n_cores, r) ## dataframe for running
jobs <- lapply(1:nrow(jobs), function(x){list(jobs=jobs[x,],seeds=round(runif(n_sim_par,0,999999999)))}) ## assign different seeds
```


## Main function

The multiplicative hazard model is
\[
\alpha(t, z) = \alpha_0(t) \prod_{k=1}^{d-1} \alpha_k(z_k).
\]

### smooth.alpha funcion

Each component \(\alpha_k\) is estimated on a grid:
\[
\{x_{k1}, x_{k2}, \dots, x_{k m_k}\}.
\]

To evaluate the function at an observed covariate value \(X_{ik}\),
a Nadaraya–Watson type kernel smoother is used:
\[
\hat{\alpha}_k(X_{ik})
=
\frac{
\sum_{\ell=1}^{m_k}
K\!\left(\frac{X_{ik}-x_{k\ell}}{b}\right)
\hat{\alpha}_k(x_{k\ell})
}{
\sum_{\ell=1}^{m_k}
K\!\left(\frac{X_{ik}-x_{k\ell}}{b}\right)
}.
\]

The baseline hazard \(\alpha_0(t)\) is treated separately because it is integrated over time
and interacts with the risk indicator \(Y_i(t)\).


Function Arguments and Their Meaning

  alpha,        # list of grid-based estimates alpha[[k]], the vector of length n.gird[k]

  K.X.b,        # list of kernel weight matrices (sample -> grid)

  k.X.b,        # list of kernel weight sums (normalization)

  K.b,          # kernel smoothing matrix for time grid (baseline)

  k.b,          # normalization vector for baseline smoothing

  b,            # bandwidth (not explicitly used here)

  n.grid,       # number of grid points per dimension

  d,            # total dimension (time + covariates)

  n,            # sample size

  nb,           # number of bandwidths (technical)

  get.baseline  # logical: whether baseline hazard is estimated



#### Role in Smooth Backfitting

`smooth.alpha()` does **not** update model parameters.  
It only provides the sample-level evaluations needed to form
\[
\prod_{j \neq k} \hat\alpha_j(X_{ij})
\]
in the smooth backfitting update equations.

#### Inputs

- Grid-based component estimates:
  \[
  \hat\alpha_k(x_{k\ell}), \quad \ell = 1,\dots,m_k,\; k=0,\dots,d-1
  \]
- Sample observations:
  \[
  X_{ik}, \quad i=1,\dots,n
  \]
- Kernel weights between samples and grids
- Logical flag `get.baseline` indicating whether the baseline hazard is estimated

---

#### Step-by-Step Procedure

##### Step 1: Initialize storage

- Create an array to store smoothed component values at sample points:
  \[
  \alpha^{\text{smooth}}_{k,i} \approx \hat\alpha_k(X_{ik})
  \]
- If the baseline hazard is estimated, allocate additional storage
  for time-grid smoothing.

---

##### Step 2: Loop over model components \(k = 0, \dots, d-1\)

###### Case A: Baseline component (\(k = 0\))

1. Set
   \[
   \alpha^{\text{smooth}}_{0,i} = 1
   \quad \text{for all } i
   \]
   since the baseline does not enter pointwise multiplicative products.
2. Smooth the baseline hazard on the **time grid** using a kernel smoother:
   \[
   \hat\alpha_0^{\text{smooth}}(t_\ell)
   =
   \sum_r W_{\ell r} \hat\alpha_0(t_r),
   \]
   where the kernel weights are normalized row-wise.

---

###### Case B: Covariate components (\(k \ge 1\))

For each sample point \(i = 1, \dots, n\):

1. Compute kernel weights between the sample value \(X_{ik}\) and grid points
   \(\{x_{k\ell}\}\).
2. Normalize the weights so they sum to one.
3. Evaluate the component at the sample point via a weighted average:
   \[
   \hat\alpha_k(X_{ik})
   =
   \sum_{\ell=1}^{m_k}
   w_{ik\ell} \hat\alpha_k(x_{k\ell}).
   \]

---

#### Step 3: Return results

- If `get.baseline = FALSE`, return only
  \[
  \{\hat\alpha_k(X_{ik})\}_{k,i}.
  \]
- If `get.baseline = TRUE`, also return the smoothed baseline hazard
  on the time grid.

---

#### Output

- Smoothed component values at sample points:
  \[
  \alpha^{\text{smooth}}_{k,i}
  \]
- (Optional) Smoothed baseline hazard on the time grid

---

### get.denominator funcion






```{r}

myparallelfunction <- function(x){
  
  smooth.alpha <- function(alpha, K_X_b,k_X_b, K_b,k_b, b, n_grid, d,n,nb, get_baseline ){
    alpha_smooth_i <- array(dim = c(d,n)) # d covariates * n
    alpha_smooth_i_0 <- array(dim = c(n,n_grid[1])) #n samples * length of time points
    for ( k in 1:d ) {
      if (k==1){
        for (i in 1:n) {
          alpha_smooth_i[k,i] <- 1
          alpha_smooth_i_0[i,] <- (K_b/k_b)%*%alpha[[k]]}  # no weight dx because they are the same and constant, can be cancelled (TODO)
      } else for (i in 1:n) {
      alpha_smooth_i[k,i] <- (K_X_b[[k]][i,]/k_X_b[[k]][i])%*%alpha[[k]] 
      }
    
    if (get_baseline==FALSE) return(alpha_smooth_i) else return(list(alpha_smooth_i = alpha_smooth_i, alpha_smooth_i_0 = alpha_smooth_i_0))
    }
  }
  
  get.denominator <- function(data, alpha, K_X_b,k_X_b, K_b,k_b, Y, kk, bb, x_grid, n_grid, d,n,nb, get_baseline){
    alpha_smooth_i <- smooth.alpha(alpha, K_X_b,k_X_b, K_b,k_b, b, n_grid, d,n,nb, get_baseline) 
    if (get_baseline==FALSE){
      alpha_minusk_smooth <- numeric(n)
      for (i in 1:n) {alpha_minusk_smooth[i] <- prod(alpha_smooth_i[-kk,i])}
    }
    if (get_baseline==TRUE){
      alpha_minusk_smooth <- array(dim=c(n,n_grid[1]))
      for (i in 1:n) {
        if (kk==1){alpha_minusk_smooth[i,] <- rep(prod(alpha_smooth_i$alpha_smooth_i[-1,i], n_grid[1]))}
      }else {alpha_minusk_smooth[i,] <- prod(alpha_smooth_i$alpha_smooth_i[-kk,i]*alpha_smooth_i$alpha_smooth_i_0)}
    }
    if (get_baseline == TRUE){
      for (i in 1:n) {
        if(kk==1){
          D <- rowSums(sapply(1:n,function(i){return(
          (c((x_grid[[1]][-1]-x_grid[[1]][-n_grid[1]]), x_grid[[1]][n_grid[1]]-x_grid[[1]][(n_grid[1]-1)])
          *(alpha_minusk_smooth[i,]*Y[i,]))
          %*%(K_b/k_b)
            )}))
        }else {
          D <- rowSums(sapply(1:n,function(i){return(
            as.numeric(
            c((x_grid[[1]][-1]-x_grid[[1]][-n_grid[1]]), x_grid[[1]][n_grid[1]]-x_grid[[1]][(n_grid[1]-1)])
            %*%(Y[i,]
            *(alpha_minusk_smooth[i,])))
            *(K_X_b[[kk]][i,]/k_X_b[[kk]][i])
            )}))
        }
      }
    }else {
      if (kk==1){
        D<- rowSums(sapply(1:n, function(i){ return(
          (c((x_grid[[1]][-1]-x_grid[[1]][-n_grid[1]]), x_grid[[1]][n_grid[1]]-x_grid[[1]][(n_grid[1]-1)])
           *(alpha_minusk_smooth[i]*Y[i,]))
           %*%(K.b/k.b))}))   # the same as when get.baseline==TRUE and kk=1
        }else D<- rowSums(sapply(1:n, function(i){ return(
          as.numeric(c((x_grid[[1]][-1]-x_grid[[1]][-n_grid[1]]), x_grid[[1]][n_grid[1]]-x_grid[[1]][(n_grid[1]-1)])
                     %*%(Y[i,] 
                     *(alpha_minusk_smooth[i])))
                     *(K_X_b[[kk]][i,]/k_X_b[[kk]][i]))}))
    }
    # O <- rowSums(sapply(1:n, function(i){ return((alpha.minusk.smooth[kk,bb,i]*data$status[i])*(K.X.b[[kk]][bb,i,]/k.X.b[[kk]][bb,i]))}))
    O <- rowSums(sapply(1:n, function(i){ return((data$status[i])*(K_X_b[[kk]][i,]/k_X_b[[kk]][i]))}))                                               # equivalent to SBF_MH_LC 
    return(O/D)
    
  }
  
  sbf <- function(x){
    
  }
  
}
```




